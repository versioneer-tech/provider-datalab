{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Provider Datalab","text":"<p>The Provider Datalab package turns Kubernetes into a platform for collaborative, cloud-native workspaces, built on Crossplane v2. It gives end users self-service access to reproducible environments for coding, data exploration, and analysis \u2014 and it gives operators a unified control plane to provision and secure those environments at scale.</p> <p>It also works seamlessly with Provider Storage, enabling S3-compatible storage provisioning and integration directly into Datalabs.</p> <p>Instead of hand-crafting Jupyter or Educates deployments, every workspace is declared through a single Kubernetes Custom Resource: the <code>Datalab</code> claim. This claim captures who should have access, whether a virtual cluster is needed, what sessions should run, and what files or datasets should be preloaded \u2014 while Crossplane and the compositions take care of provisioning all the moving parts.</p> <p>For end users, this means:</p> <ul> <li>Launch personal or shared analysis environments with one manifest.  </li> <li>Get preconfigured access to storage, credentials, and workshop material.  </li> <li>Work inside familiar tools like VS Code Server, JupyterLab, or terminals, bundled with utilities such as <code>awscli</code> and <code>rclone</code>.  </li> </ul> <p>For operators, this means:</p> <ul> <li>A consistent, declarative model for managing heterogeneous runtime stacks.  </li> <li>Automated provisioning of vclusters, identity integration via Keycloak, and storage connections.  </li> <li>Extensibility to plug in additional runtimes or policies without changing the user-facing API.  </li> </ul> <p>At its core, Provider Datalab provides:</p> <ul> <li>A Datalab Composite Resource Definition (XRD) </li> <li>Compositions powered by Crossplane v2 to provision environments with storage, sessions, vclusters, and identity wiring  </li> <li>Seamless integration of authentication and access control  </li> </ul> <p>With Provider Datalab, workspaces become declarative, multi-tenant, and self-service, while operators retain full control over identity, security, and resource governance.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Workspace abstraction   Define and provision full-featured data labs based on Educates or Jupyter as a single resource.  </li> <li>Multi-tenant support   Each Datalab can run isolated inside a Kubernetes namespace or in a dedicated virtual cluster (vcluster).  </li> <li>Integrated identity   Seamless authentication and authorization via Keycloak.  </li> <li>Declarative storage   Provision and attach buckets with access policies.  </li> <li>Extensible by design   Built on Crossplane, ready to extend with new resources.  </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the configuration package into your Crossplane environment, e.g. based on Educates, use:</p> <pre><code>apiVersion: pkg.crossplane.io/v1\nkind: Configuration\nmetadata:\n  name: datalab-educates\nspec:\n  package: ghcr.io/versioneer-tech/provider-datalab/eductaes0.2.0\n  skipDependencyResolution: true\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#minimal-example","title":"Minimal Example","text":"<pre><code>apiVersion: pkg.internal/v1beta1\nkind: Datalab\nmetadata:\n  name: team-wonderland\nspec:\n  users:\n  - alice\n  sessions:\n  - default\n  vcluster: true\n</code></pre> <p>This provisions a vcluster within a dedicated Kubernetes namespace and starts the Educates tooling stack (including VS Code Server and a terminal), together with bundled utilities. A storage browser is available with storage automatically mounted, and additional tools such as <code>awscli</code> and <code>rclone</code> are preinstalled to support typical data lab tasks like coding, data exploration, and wrangling.  </p> <p>Access to the datalab is restricted to Alice, since she currently is the only user associated with this lab.  </p> <p>Combined with a small, cluster-specific <code>EnvironmentConfig</code> (realm, ingress domain/class, storage secret), the platform handles the rest\u2014provisioning the chosen runtime, mounting credentials, and preloading content.  </p> <p>Note</p> <p>All configuration packages built from <code>provider-datalab</code> (educates, jupyter,...) share the same Composite Resource Definition!</p>"},{"location":"#more-examples","title":"More Examples","text":"<p>Check the examples folder in the GitHub repository for complete scenarios, including: - Datalabs with multiple users - Datalabs with integrated storage - Identity-aware environments</p>"},{"location":"how-to-guides/additional_services/","title":"Additional Services","text":"<p>This section explains how end users can extend the default <code>provider-datalab</code> capabilities by deploying additional services and tools that support their daily workflows.</p> <p>A <code>Datalab</code> environment provides a preconfigured VS Code Server with a persistent file system and access to the connected object storage, along with essential CLI tools such as <code>git</code>, <code>curl</code>, <code>aws</code>, or <code>rclone</code>. While this already covers many data exploration and transformation needs, users often require more specialized tooling \u2014 for example, dashboards for visualization, services for experiment tracking, or out-of-process compute backends for scalable data processing.</p> <p>Although many of these tools can be started directly from the integrated terminal and exposed via VS Code\u2019s port forwarding feature, that approach tends to be fragile and transient - you must carefully manage Python environments, avoid breaking dependencies during upgrades, and remember that the terminal session lifetime is temporary.</p> <p>A more robust approach is to deploy such services as native Kubernetes applications \u2014 directly from within the Datalab. Because each Datalab session has access to the Kubernetes API (depending on the operator configuration), users can deploy workloads within their assigned namespace or, when running in vCluster mode, inside a fully isolated virtual cluster with their own CRDs, RBAC rules, and controllers.  This enables running even complex frameworks that typically require cluster-wide resources \u2014 for example, a Dask Gateway.</p> <p>Note: The <code>kubectl</code> and <code>helm</code> CLIs are preinstalled as well. You can apply manifests, install Helm charts, and inspect Kubernetes resources directly from the terminal.</p>"},{"location":"how-to-guides/additional_services/#example-deploying-a-dask-cluster","title":"Example: Deploying a Dask Cluster","text":"<p>The following example shows how to start a simple Dask scheduler and worker deployment directly inside your Datalab namespace. This provides a minimal distributed compute backend that you can connect to from Python via <code>dask.distributed.Client</code>.</p> Click to expand: Deploy Dask <pre><code>kubectl apply -f - &lt;&lt;'EOF'\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: dask-scheduler\nspec:\n  selector:\n    app: dask-scheduler\n  ports:\n    - name: tcp-scheduler\n      port: 8786\n      targetPort: 8786\n    - name: http-dashboard\n      port: 8787\n      targetPort: 8787\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dask-scheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dask-scheduler\n  template:\n    metadata:\n      labels:\n        app: dask-scheduler\n    spec:\n      containers:\n        - name: scheduler\n          image: daskdev/dask:2025.4.0\n          args: [\"dask-scheduler\", \"--dashboard-address\", \":8787\"]\n          ports:\n            - containerPort: 8786\n            - containerPort: 8787\n          resources:\n            requests: {cpu: \"500m\", memory: \"1Gi\"}\n            limits:   {cpu: \"1\",    memory: \"2Gi\"}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dask-worker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dask-worker\n  template:\n    metadata:\n      labels:\n        app: dask-worker\n    spec:\n      containers:\n        - name: worker\n          image: daskdev/dask:2025.4.0\n          args: [\"dask-worker\", \"tcp://dask-scheduler:8786\", \"--nthreads\", \"2\", \"--memory-limit\", \"2GB\"]\n          resources:\n            requests: {cpu: \"500m\", memory: \"1Gi\"}\n            limits:   {cpu: \"1\",    memory: \"2Gi\"}\nEOF\n</code></pre> <p>Once running, you can port-forward and use the VS Code Ports tab to explore the Dask dashboard:</p> <pre><code>kubectl port-forward svc/dask-scheduler 8787:8787\n</code></pre> <p>You can also deploy Dask Gateway via Helm \u2014 this is only possible in vCluster mode, since it requires cluster-wide resources such as CRDs and RBAC cluster roles:</p> <pre><code>helm repo update\nhelm upgrade --install dask-gateway dask/dask-gateway   -n \"${DEFAULT_NAMESPACE:-default}\"   --create-namespace   --set gateway.auth.type=simple   --set gateway.auth.simple.password=''   --set traefik.service.type=ClusterIP   --set gateway.backend.image.name=ghcr.io/dask/dask-gateway   --set gateway.backend.image.tag=2025.4.0   --wait --atomic\n</code></pre>"},{"location":"how-to-guides/additional_services/#example-deploying-mlflow-with-persistent-storage","title":"Example: Deploying MLflow with Persistent Storage","text":"<p><code>MLflow</code> is a popular experiment-tracking platform that complements data exploration workflows. The following example deploys an MLflow server together with a simple SQLite backend and a <code>PersistentVolumeClaim</code> for artifact and metadata storage.</p> <p>Note: The PVC is bound to your Datalab session. Once the Datalab is deleted, the PVC and stored data will also be removed unless your operator configures a persistent storage backend.</p> Click to expand: Deploy MLflow <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-creds\ntype: Opaque\nstringData:\n  accessKey: \"${AWS_ACCESS_KEY_ID}\"\n  accessSecret: \"${AWS_SECRET_ACCESS_KEY}\"\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: minio-config\ndata:\n  endpoint: \"${AWS_ENDPOINT_URL}\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mlflow\nspec:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow\nspec:\n  selector:\n    app: mlflow\n  ports:\n    - name: http\n      port: 5000\n      targetPort: 5000\n      protocol: TCP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mlflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlflow\n  template:\n    metadata:\n      labels:\n        app: mlflow\n    spec:\n      containers:\n        - name: mlflow\n          image: ghcr.io/mlflow/mlflow:latest\n          command: [\"/bin/sh\",\"-lc\"]\n          args:\n            - |\n              python -m pip install --no-cache-dir --upgrade pip &amp;&amp;\n              pip install --no-cache-dir boto3 &amp;&amp;\n              exec mlflow server \\\n                --backend-store-uri sqlite:////mlflow/mlflow.db \\\n                --serve-artifacts \\\n                --artifacts-destination s3://ws-frank/mlruns \\\n                --host 0.0.0.0 --port 5000 \\\n                --workers 2 \\\n                --allowed-hosts '*' \\\n                --cors-allowed-origins '*'\n          ports:\n            - containerPort: 5000\n          resources:\n            requests: { cpu: \"100m\", memory: \"512Mi\" }\n            limits:   { cpu: \"300m\", memory: \"2Gi\" }\n          env:\n            - name: MLFLOW_S3_ENDPOINT_URL\n              valueFrom:\n                configMapKeyRef: { name: minio-config, key: endpoint }\n            - name: AWS_ACCESS_KEY_ID\n              valueFrom:\n                secretKeyRef: { name: minio-creds, key: accessKey }\n            - name: AWS_SECRET_ACCESS_KEY\n              valueFrom:\n                secretKeyRef: { name: minio-creds, key: secretKey }\n            - name: AWS_S3_FORCE_PATH_STYLE\n              value: \"true\"\n            - name: AWS_EC2_METADATA_DISABLED\n              value: \"true\"\n          volumeMounts:\n            - name: data\n              mountPath: /mlflow\n          readinessProbe:\n            httpGet: { path: \"/\", port: 5000 }\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet: { path: \"/\", port: 5000 }\n            initialDelaySeconds: 20\n            periodSeconds: 20\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: mlflow\nEOF\n</code></pre> <p>Once running, you can port-forward and use the VS Code Ports tab to explore the MLflow UI:</p> <pre><code>kubectl port-forward svc/mlflow 5000:5000\n</code></pre> <p>To use MLflow in your code, you need to connect to the tracking server running at <code>http://localhost:5000</code>. This can be done by setting the following environment variable:</p> <pre><code>export MLFLOW_TRACKING_URI=\"http://127.0.0.1:5000\"\n</code></pre>"},{"location":"how-to-guides/additional_services/#summary","title":"Summary","text":"<p>In its current form, <code>provider-datalab</code> focuses on deploying ephemeral or stateless services on Kubernetes in a seamless and reproducible way. These services are tied to the Datalab session lifecycle, ensuring automatic cleanup and cost efficiency when sessions are terminated.</p> <p>However, if your operator provides additional storage capabilities \u2014 for example:</p> <ul> <li>persistent block storage (via Kubernetes <code>StorageClass</code>)</li> <li>relational databases (PostgreSQL, MySQL)</li> <li>key\u2013value stores (Redis, etcd)</li> </ul> <p>then more complex, stateful workloads can also be supported. Such setups, however, come with additional maintenance effort and require clear alignment of responsibilities between operators and end users.  Making these integrations easier and more declarative is planned for future releases.</p>"},{"location":"how-to-guides/installation/","title":"Provider Datalab \u2013 Installation Guide","text":"<p>The <code>provider-datalab</code> configuration packages let you provision collaborative data labs on Kubernetes, using either Educates or Jupyter runtimes. Labs, sessions, storage, and identity are declared via a single, namespaced <code>Datalab</code> spec.</p>"},{"location":"how-to-guides/installation/#namespacing-model-important","title":"Namespacing Model (Important)","text":"<p>Everything in this guide is namespaced:</p> <ul> <li>You apply <code>Datalab</code> claims to a namespace (e.g., <code>workspace</code>).  </li> <li>The referenced Secret for storage lives in the same namespace as the <code>Datalab</code> claim (Secret name = <code>spec.secretName</code>).  </li> <li>Any namespaced ProviderConfigs or supporting objects that the compositions depend on must exist in that same target namespace (e.g., <code>workspace</code>).  </li> </ul> <p>In short: choose your target namespace (e.g., <code>workspace</code>), apply the provider configs there, and create your <code>Datalab</code> claims in that namespace.</p>"},{"location":"how-to-guides/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (e.g., <code>kind</code>, managed K8s).  </li> <li><code>kubectl</code> access.  </li> <li>Crossplane installed in the cluster:  </li> </ul> <pre><code>helm repo add crossplane-stable https://charts.crossplane.io/stable\nhelm repo update\nhelm install crossplane \\\n  --namespace crossplane-system \\\n  --create-namespace crossplane-stable/crossplane \\\n  --version 2.0.2 \\\n  --set provider.defaultActivations={}\n</code></pre> <ul> <li>Educates installed with all CRDs in the cluster if you plan to use the Educates runtime.   See the Educates Installation Guide for details.  </li> <li>JupyterHub / Jupyter Operator installed if you plan to use the Jupyter runtime (upcoming integration).  </li> </ul> <p>Without the corresponding runtime installed, Datalab claims for that variant will not reconcile.</p> <p>To reduce control-plane load, we use a <code>ManagedResourceActivationPolicy</code> (MRAP) per backend so only the needed Managed Resources are active.</p>"},{"location":"how-to-guides/installation/#step-1-install-provider-dependencies-per-runtime","title":"Step 1 \u2013 Install Provider Dependencies (per runtime)","text":"<p>All runtimes follow the same staged pattern you must install before the configuration package: 1. ManagedResourceActivationPolicy \u2013 activate only the resource kinds that are needed. 2. Deployment Runtime Configs \u2013 define how providers/functions run. 3. Providers \u2013 install the required Crossplane providers. 4. ProviderConfigs (namespaced) \u2013 configure providers in your target namespace. 5. Functions \u2013 install supporting Crossplane Functions. 6. RBAC \u2013 permissions for <code>provider-kubernetes</code> to observe and reconcile objects.</p> <p>Repository root: https://github.com/versioneer-tech/provider-datalab/</p>"},{"location":"how-to-guides/installation/#educates-runtime","title":"Educates Runtime","text":"<p>You operate the Educates training platform in your cluster. Ensure Educates is installed with all CRDs before proceeding.  </p> <p>Provider dependencies activate Helm, Kubernetes, and Keycloak resources as needed:</p> <ul> <li>00-mrap.yaml \u2013 Activate Educates-specific Managed Resources.  </li> <li>01-deploymentRuntimeConfigs.yaml \u2013 Runtime configs for providers/functions.  </li> <li>02-providers.yaml \u2013 Install Helm, Kubernetes, and Keycloak providers.  </li> <li>03-providerConfigs.yaml \u2013 Apply in your target namespace (e.g., <code>workspace</code>); sets up storage and identity configs.  </li> <li>functions.yaml \u2013 Functions used by compositions.  </li> <li>rbac.yaml \u2013 RBAC for <code>provider-kubernetes</code>.</li> </ul> <p>When installed, a Datalab will provision a vcluster (if enabled) and launch the Educates tooling stack (VS Code Server, terminal, storage browser, plus preinstalled tools like <code>awscli</code> and <code>rclone</code>).</p>"},{"location":"how-to-guides/installation/#jupyter-runtime-upcoming","title":"Jupyter Runtime (upcoming)","text":"<p>Upcoming integration!</p>"},{"location":"how-to-guides/installation/#step-2-install-the-configuration-package-after-dependencies","title":"Step 2 \u2013 Install the Configuration Package (after dependencies)","text":"<p>Once the provider dependencies are in place, install the configuration package for your chosen runtime. This registers the <code>Datalab</code> CRD and compositions and allows immediate reconciliation because the providers/configs already exist.</p> <p>Example \u2013 Educates</p> <pre><code>apiVersion: pkg.crossplane.io/v1\nkind: Configuration\nmetadata:\n  name: datalab-educates\nspec:\n  package: ghcr.io/versioneer-tech/provider-datalab/educates:0.2.0\n  skipDependencyResolution: true\n</code></pre> <p>Apply:</p> <pre><code>kubectl apply -f configuration.yaml\n</code></pre>"},{"location":"how-to-guides/installation/#step-3-environment-configuration","title":"Step 3 \u2013 Environment configuration","text":"<p>Provide cluster-specific settings through an <code>EnvironmentConfig</code>. The composition consumes this to render ingress, identity, and storage correctly:</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1beta1\nkind: EnvironmentConfig\nmetadata:\n  name: datalab\ndata:\n  iam:\n    realm: demo\n  ingress:\n    class: nginx\n    domain: datalab.demo\n    secret: wildcard-tls\n  storage:\n    endpoint: https://s3.demo\n    provider: Other\n    region: demo\n    force_path_style: \"true\"\n    secretNamespace: datalab\n    type: s3\n  network:\n    serviceCIDR: \"10.43.0.0/16\"\n</code></pre>"},{"location":"how-to-guides/installation/#step-4-storage-credentials","title":"Step 4 \u2013 Storage credentials","text":"<p>The <code>storage</code> section in the <code>EnvironmentConfig</code> references a Kubernetes Secret \u2014 named identically to <code>spec.secretName</code> in the Datalab \u2014 which must already exist in the cluster. This Secret must reside in the namespace specified by <code>storage.secretNamespace</code> and include at least:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code> </li> <li><code>AWS_SECRET_ACCESS_KEY</code> </li> </ul> <p>Create it manually, for example:</p> <pre><code>kubectl -n datalab create secret generic demo \\\n  --from-literal=AWS_ACCESS_KEY_ID=&lt;KEY_ID&gt; \\\n  --from-literal=AWS_SECRET_ACCESS_KEY=&lt;SECRET&gt;\n</code></pre>"},{"location":"how-to-guides/installation/#step-5-create-a-datalab","title":"Step 5 \u2013 Create a Datalab","text":"<p>The minimal example creates a user-scoped lab with one session. - Sessions present \u2192 a runtime is automatically started until stopped by the operator. - No sessions \u2192 no runtime is started until the user explicitly launches one. - Files present \u2192 workshop tab enabled; none \u2192 no workshop tab. - <code>spec.vcluster: true</code> \u2192 vcluster provisioned; <code>false</code> \u2192 namespace-scoped runtime.</p> <pre><code>apiVersion: pkg.internal/v1beta1\nkind: Datalab\nmetadata:\n  name: demo\n  namespace: datalab\nspec:\n  users:\n  - alice\n  sessions:\n  - default\n  vcluster: true\n  files: []\n</code></pre> <p>For more scenarios, see these <code>example manifests</code>, which demonstrate: - labs with multiple users - enabling/disabling <code>spec.vcluster</code> - attaching workshop files from Git, OCI images, or HTTP sources  </p>"},{"location":"how-to-guides/installation/#step-6-validate-installation","title":"Step 6 \u2013 Validate installation","text":"<p>Check that packages, providers, CRDs, and your XRD are healthy:</p> <pre><code>kubectl get providers.pkg.crossplane.io\nkubectl get providerrevisions.pkg.crossplane.io\n\nkubectl get configurations.pkg.crossplane.io\nkubectl get configurationrevisions.pkg.crossplane.io\n\nkubectl api-resources --api-group=kubernetes.crossplane.io\nkubectl api-resources --api-group=helm.crossplane.io\nkubectl api-resources --api-group=keycloak.crossplane.io\n\nkubectl get managedresourcedefinitions | grep -E 'helm|kubernetes|keycloak'\n\nkubectl get xrd\nkubectl get datalabs.pkg.internal -A\n</code></pre>"},{"location":"how-to-guides/security/","title":"Provider Datalab \u2013 Security in Cloud Workspaces","text":"<p>Running Datalabs means letting end users run their own code on your servers. In practice, this is like operating a shared computer lab: some users are trusted colleagues, others are students, and some may be external partners. Everyone is authenticated, but authentication is not the same as trust. An authenticated user can still make mistakes, run unsafe workloads, or attempt to break out of their sandbox. The security model of Datalabs must therefore acknowledge different trust levels: users can be trusted to log in, but not necessarily trusted with unrestricted access.</p> <p>Out of the box, our platform applies a baseline security model: workloads are isolated into namespaces, Pods are subject to Kubernetes admission controls, and NetworkPolicies block access to sensitive metadata endpoints. This baseline is pragmatic but effective. Beyond it, security can be extended with Kyverno policies and other admission controllers to enforce stricter boundaries.</p>"},{"location":"how-to-guides/security/#configurable-security-options","title":"Configurable Security Options","text":"<p>Datalabs expose configurable runtime security through the <code>spec.security</code> section of the <code>Datalab</code> resource. This allows operators to adjust the trust level per environment:</p> <ul> <li><code>policy</code> defines the Pod Security Standard (<code>restricted</code>, <code>baseline</code>, <code>privileged</code>).  </li> <li><code>restricted</code> \u2192 strictest: disallows most privileges.  </li> <li><code>baseline</code> \u2192 safe default: blocks host access and elevated privileges.  </li> <li><code>privileged</code> \u2192 relaxed: allows Docker-in-Docker with 20 Gi of local storage.  </li> <li><code>kubernetesAccess</code> toggles whether a Kubernetes API token is mounted in the session pod. When disabled, users can run workloads but not interact with the cluster API.  </li> <li><code>kubernetesRole</code> sets the RBAC level (<code>view</code>, <code>edit</code>, <code>admin</code>) within the Datalab namespace or vcluster.</li> </ul>"},{"location":"how-to-guides/security/#network-isolation-with-networkpolicies","title":"Network Isolation with NetworkPolicies","text":"<p>Kubernetes NetworkPolicies control how pods communicate with each other and with external services. By default, a cluster allows all pod-to-pod and internet traffic. NetworkPolicies reverse this: once isolation is enabled, only explicitly allowed traffic flows.</p> <p>Policies are flexible: they can be as strict as \u201cthis pod only talks to another on one port,\u201d or as permissive as \u201call pods in this namespace can reach the internet.\u201d What we apply here is not a perfect \u201czero trust\u201d design but a pragmatic baseline balancing usability and protection.</p> <p>When a Datalab is created, NetworkPolicies are provisioned automatically in the corresponding namespace:</p> <ul> <li>Egress rules apply to pods in the namespace (the traffic source).  </li> <li>Ingress rules apply to pods in the namespace (the traffic destination).  </li> </ul>"},{"location":"how-to-guides/security/#allow-web-egress","title":"allow-web-egress","text":"<p>Currently, we deploy a single permissive policy:</p> <ul> <li>Pods: all pods in environment namespaces (where user workloads run)  </li> <li>Allows: all egress traffic to the internet  </li> <li>Blocks: access to the cloud instance metadata endpoint <code>169.254.169.254/32</code> </li> </ul> <p>This endpoint (known as IMDS) provides cloud instance credentials that bypass Kubernetes RBAC if exposed inside a pod. Protecting it is critical in multi-tenant setups.  </p> <p>Note: In our IPv4-only cluster, only <code>169.254.169.254</code> must be blocked. In dual-stack or IPv6-only clusters, the corresponding IPv6 endpoint (commonly <code>fd00:ec2::254</code>) must also be restricted.</p> <p>This \u201callow everything except IMDS\u201d approach ensures users can fetch packages, clone repositories, or access S3 buckets, while removing the most dangerous privilege escalation path.</p>"},{"location":"how-to-guides/security/#vcluster-vs-namespace-security","title":"vcluster vs Namespace Security","text":"<p>vcluster improves the developer experience by giving each tenant a virtual control plane, but it does not create stronger isolation:</p> <ul> <li>All workloads still run in the host namespace.  </li> <li>PodSecurity and NetworkPolicies apply at the host namespace level.  </li> <li>vcluster mirrors host cluster policies: it does not itself prevent privileged workloads (<code>hostPath</code>, <code>hostNetwork</code>, <code>privileged: true</code>) unless these are already disallowed at cluster level.  </li> </ul> <p>Takeaway: treat vcluster namespaces the same as direct tenant namespaces. Enable Pod Security Admission (<code>baseline</code> or <code>restricted</code>) and rely on NetworkPolicies for communication boundaries.</p>"},{"location":"how-to-guides/security/#dns-architecture-in-workshop-environments","title":"DNS Architecture in Workshop Environments","text":"<p>For workshop-style environments that spin up many short-lived vclusters, starting a full DNS service inside every vcluster would slow down startup and waste resources. Instead, a shared DNS service in the host cluster handles name resolution for all vclusters.</p> <p>When a vcluster is created, a small CoreDNS Service and Deployment is automatically deployed in the host cluster (for example <code>kube-dns-x-kube-system-x-my-vcluster</code>). Pods inside the vcluster still use normal Kubernetes DNS names like <code>kube-dns.kube-system.svc.cluster.local</code>, but those lookups are transparently routed to the host-level DNS service. That host DNS server then talks to the vcluster\u2019s API to resolve internal service names.</p> <p>Advantages: - Fast startup \u2014 no DNS bootstrap delay per vcluster - Lower resource use \u2014 one lightweight host DNS handles many environments - Simpler networking \u2014 all workloads share the same cluster network - Central visibility \u2014 DNS logging and policies stay managed in one place  </p> <p>From the user\u2019s point of view, everything behaves as expected: pods inside each workshop can resolve service names normally, without needing to know that DNS is handled outside their vcluster. However, some cloud environments differ in how their CNI plugins route service traffic, which can affect how the host DNS reaches the vcluster API. In such cases, minor adjustments to NetworkPolicies or CoreDNS endpoints may be required to restore internal name resolution.</p>"},{"location":"how-to-guides/security/#policy-enforcement","title":"Policy Enforcement","text":"<p>The enforcement of NetworkPolicies depends on the CNI plugin (Cilium, Calico, Antrea, etc.). Without a CNI that supports them, policies have no effect.</p> <p>We assume:</p> <ul> <li>CNI enforces policies consistently across all nodes.  </li> <li>IMDS endpoints are blocked via the allow-web-egress policy.  </li> <li>External access flows through a central ingress controller for TLS, auditing, and routing.  </li> </ul> <p>More restrictive egress controls (e.g., whitelisting PyPI, GitHub, or S3) could be added, but these create significant operational overhead and reduce usability. Our design is intentionally permissive, with the option to tighten later.</p> <p>Kyverno can be added to enforce stricter pod-level policies across environments. For example, you can block privileged pods, disallow host networking, or restrict image registries \u2014 preventing workloads that could otherwise compromise the host cluster.</p>"},{"location":"how-to-guides/security/#summary","title":"Summary","text":"<p>In our current configuration, the baseline security model is:</p> <ul> <li>Apply one permissive egress policy per environment namespace: allow everything, except block the cloud metadata endpoint.  </li> <li>Use PodSecurity Admission to prevent privilege escalation.  </li> <li>Expose security knobs per Datalab:  </li> <li>Enable or disable Kubernetes API access (<code>kubernetesAccess</code>).  </li> <li>Configure role-based privileges (<code>kubernetesRole</code>).  </li> <li>Select an overall Pod Security profile (<code>restricted</code>, <code>baseline</code>, <code>privileged</code>).  </li> <li>Keep policies in the environment namespace, where workloads actually run.  </li> <li>Route ingress through a central controller for TLS and auditing.  </li> </ul> <p>This setup prioritizes usability for researchers while protecting the cluster\u2019s most critical boundary: preventing pods from stealing cloud instance credentials.  </p> <p>Over time, this model can evolve into stricter egress filtering or finer-grained ingress/egress rules, but it already provides a safe and pragmatic baseline for multi-tenant Datalabs.</p>"},{"location":"how-to-guides/usage_concepts/","title":"Provider Datalab \u2013 Usage &amp; Concepts","text":"<p>This section explains how to use the <code>provider-datalab</code> configuration packages once they are installed. It focuses on the concepts of Sessions, Files, vclusters, Storage Secrets, and the required Keycloak integration for identity and access.</p>"},{"location":"how-to-guides/usage_concepts/#concepts","title":"Concepts","text":""},{"location":"how-to-guides/usage_concepts/#sessions","title":"Sessions","text":"<p>A <code>Datalab</code> claim may declare one or more <code>spec.sessions</code>.  </p> <ul> <li>If at least one session is listed, a corresponding WorkshopSession is automatically created and will run permanently until stopped by the operator.  </li> <li>If no sessions are given, no runtime will be started; users must explicitly launch a session themselves (<code>auto</code> mode).  </li> </ul> <p>Sessions can also be patched into the spec later if needed.</p>"},{"location":"how-to-guides/usage_concepts/#persistence","title":"Persistence","text":"<p>Each <code>Datalab</code> session is equipped with a persistent volume for storing files, in addition to the connected object storage.  This ensures that user data and session state are preserved even if the workshop pod is restarted or rescheduled by Kubernetes.  Installing code libraries, handling metadata, or working with Git repositories often generates many small files that may be updated frequently. A storage class providing NFS-like capabilities is usually a good fit for these kinds of workloads, object storage abstractions are not.</p> <p>The persistent volume claim (PVC) is tied to the active session and will be deleted automatically when the workshop session shuts down (for example, through a culling process when using session mode <code>auto</code>). This does not necessarily mean that data is lost \u2014 when the session is restarted from the same manifests, Kubernetes will recreate the PVC with the same name, reattaching it to the existing data in environments that use an NFS server or another shared storage backend, since the PVC will point to the same physical folder.</p> <p>This behavior works as long as the associated <code>StorageClass</code> has its <code>reclaimPolicy</code> set to <code>Retain</code> (not <code>Delete</code>), ensuring that data is not removed externally. It also depends on maintaining a consistent link between the PVC name and the actual storage path.  If the underlying storage system assigns randomized volume identifiers (such as UIDs for folder paths), the data will still remain on the storage backend after the session ends, but Kubernetes will not automatically reattach it to a new PVC \u2014 manual reassociation may then be required.</p>"},{"location":"how-to-guides/usage_concepts/#authentication","title":"Authentication","text":"<p>Access to a <code>Datalab</code> session is restricted, with the environment configuration determining the authentication strategy. By default, the same credentials used to access the connected object storage buckets are also applied for session login. Authentication can be globally disabled by setting <code>auth.type = none</code>, for example, in air-gapped environments or when access is already secured at the ingress level through other mechanisms.  </p> <p>Each <code>Datalab</code> automatically provisions a dedicated Keycloak OAuth2 client, which can be used to protect the session using standard OIDC flows. Full integration and automated configuration of this setup are planned for future releases.</p>"},{"location":"how-to-guides/usage_concepts/#files-and-the-workshop-tab","title":"Files and the Workshop Tab","text":"<p>The <code>spec.files</code> array is optional.  </p> <ul> <li>When empty or omitted, no workshop tab is rendered in the Educates UI.  </li> <li>When at least one source is defined, workshop and/or data content is mounted and the tab is enabled.</li> </ul> <p>Supported sources:</p> <ul> <li>OCI image (<code>spec.files[].image</code>)  </li> <li>Git repository (<code>spec.files[].git</code>)  </li> <li>HTTP(S) download (<code>spec.files[].http</code>)  </li> </ul> <p>Filters (<code>includePaths</code>, <code>excludePaths</code>, <code>newRootPath</code>, <code>path</code>) control what ends up visible.</p>"},{"location":"how-to-guides/usage_concepts/#vcluster-toggle","title":"vcluster toggle","text":"<p><code>spec.vcluster</code> is a boolean flag. - <code>true</code> \u2192 the datalab provisions a vcluster for runtime isolation. - <code>false</code> \u2192 workloads run directly in the namespace.</p>"},{"location":"how-to-guides/usage_concepts/#storage-secret","title":"Storage Secret","text":"<p>A Datalab requires credentials to an S3-compatible storage system. Credentials are expected to exist in a Kubernetes Secret named via <code>spec.secretName</code>, in the same namespace as the Datalab.  </p> <p>This secret must include at least the <code>access_key</code> and <code>access_secret</code>. The endpoint and provider are defined in <code>EnvironmentConfig.data.storage</code>.</p>"},{"location":"how-to-guides/usage_concepts/#security-and-access-policy","title":"Security and Access Policy","text":"<p>The <code>spec.security</code> section controls access permissions and runtime privilege level for sessions.</p> <p>Key fields:</p> <ul> <li><code>policy</code> \u2014 defines Pod Security Standard (<code>restricted</code>, <code>baseline</code>, <code>privileged</code>).  </li> <li><code>privileged</code> enables Docker-in-Docker with 20 Gi of local storage.  </li> <li><code>kubernetesAccess</code> \u2014 whether a Kubernetes service account token is mounted inside the session.  </li> <li><code>kubernetesRole</code> \u2014 defines in-namespace RBAC level (<code>admin</code>, <code>edit</code>, <code>view</code>).  </li> </ul>"},{"location":"how-to-guides/usage_concepts/#resource-quotas","title":"Resource Quotas","text":"<p>The <code>spec.quota</code> section allows per-Datalab overrides of default compute and storage budgets.  </p> <ul> <li><code>memory</code> \u2014 memory allocation per session (default 2 Gi).  </li> <li><code>storage</code> \u2014 persistent volume size (default 1 Gi).  </li> <li><code>budget</code> \u2014 Educates resource budget profile (<code>small</code>, <code>medium</code>, <code>large</code>, <code>x-large</code>, etc.).  </li> </ul> <p>When unspecified, defaults from the EnvironmentConfig apply.</p> Budget CPU Memory small 1000m 1Gi medium 2000m 2Gi large 4000m 4Gi x-large 8000m 8Gi xx-large 8000m 12Gi xxx-large 8000m 16Gi"},{"location":"how-to-guides/usage_concepts/#identity-and-keycloak-resources","title":"Identity and Keycloak Resources","text":"<p>Users listed under <code>spec.users</code> must already exist in Keycloak. When a Datalab is created, the composition also provisions the required Keycloak resources:</p> <ul> <li>A Group for the datalab  </li> <li>Group memberships for the listed users  </li> <li>A Client and Role to protect access to the datalab and installed tooling  </li> </ul> <p>This ensures that authentication and authorization are consistently enforced across the runtime and UI.</p>"},{"location":"how-to-guides/usage_concepts/#example-joe-no-session-by-default","title":"Example: Joe (no session by default)","text":"<pre><code># Joe gets a personal datalab s-joe with no pre-created session.\n# He must explicitly start a session himself; nothing is running by default.\n# No vcluster is provisioned and no workshop files are attached.\n# Credentials to storage are expected to exist in a secret \"joe\" in the same namespace.\n# A Keycloak group, role, and client are created; user \"joe\" must exist in Keycloak.\napiVersion: pkg.internal/v1beta1\nkind: Datalab\nmetadata:\n  name: s-joe\nspec:\n  users:\n  - joe\n  secretName: joe\n</code></pre> <ul> <li>Joe\u2019s Datalab exists but is idle until he launches a session.  </li> <li>Useful for lightweight, on-demand environments.  </li> <li>Keycloak ensures Joe is authorized to access his workspace.  </li> </ul>"},{"location":"how-to-guides/usage_concepts/#example-jeff-and-jim-shared-session-privileged-with-docker","title":"Example: Jeff and Jim (shared session, privileged with Docker)","text":"<pre><code># Jeff and Jim share a datalab s-jeff with one default shared session\n# automatically created. That session will run permanently until stopped by the operator.\n# The lab does not use a vcluster and has no workshop files.\n# Credentials to storage are expected to exist in a secret \"jeff\" in the same namespace.\n# A Keycloak group, role, and client are created; users \"jeff\" and \"jim\" must exist in Keycloak.\n# This configuration runs the lab in privileged mode:\n# - Security policy: \"privileged\" \u2192 automatically enables Docker with 20 Gi workspace storage.\n# - Kubernetes API access is disabled (kubernetesAccess=false).\napiVersion: pkg.internal/v1beta1\nkind: Datalab\nmetadata:\n  name: s-jeff\nspec:\n  users:\n  - jeff\n  - jim\n  secretName: jeff\n  sessions:\n  - default\n  vcluster: false\n  security:\n    policy: privileged\n    kubernetesAccess: false\n  quota:\n    memory: 4Gi\n    storage: 5Gi\n    budget: large\n  files: []\n</code></pre> <ul> <li>A long-running session is started immediately, shared by both users.  </li> <li>Runs in privileged mode with Docker support and increased ephemeral disk (20 Gi).  </li> <li>No Kubernetes API access is granted inside the environment.  </li> <li>Access is secured through the corresponding Keycloak group and role.  </li> </ul>"},{"location":"how-to-guides/usage_concepts/#example-jane-isolated-vcluster-with-admin-role-and-higher-quota","title":"Example: Jane (isolated vcluster with admin role and higher quota)","text":"<pre><code># Jane runs a datalab s-jane with a default session automatically created.\n# That session will run permanently until stopped by the operator,\n# and a dedicated vcluster is provisioned for runtime isolation.\n# No workshop files are attached. Credentials to storage are expected\n# to exist in a secret \"jane\" in the same namespace.\n# A Keycloak group, role, and client are created; user \"jane\" must exist in Keycloak.\n# This configuration explicitly overrides default resource quotas and security settings:\n# - Session quota: increased to 4 Gi memory, 10 Gi storage, budget class \"x-large\".\n# - Kubernetes role: elevated to \"admin\" for full namespace permissions.\napiVersion: pkg.internal/v1beta1\nkind: Datalab\nmetadata:\n  name: s-jane\nspec:\n  users:\n  - jane\n  secretName: jane\n  sessions: \n  - default\n  vcluster: true\n  security:\n    policy: baseline\n    kubernetesAccess: true\n    kubernetesRole: admin\n  quota:\n    memory: 8Gi\n    storage: 10Gi\n    budget: x-large\n</code></pre> <ul> <li>Jane\u2019s workloads run inside an isolated virtual cluster (<code>vcluster: true</code>).  </li> <li>The admin role grants full control within her namespace/vcluster.  </li> <li>Increased quota provides additional compute and storage capacity.  </li> <li>Suitable for advanced development or testing requiring full Kubernetes control.  </li> <li>Keycloak enforces role-based access protection for this lab.  </li> </ul>"},{"location":"how-to-guides/usage_concepts/#example-john-with-git-based-workshop-files","title":"Example: John (with Git-based workshop files)","text":"<pre><code># John has a datalab s-john with a default session automatically created.\n# That session will run permanently until stopped by the operator.\n# No vcluster is provisioned. Workshop and data files are pulled from Git,\n# enabling the workshop tab in the Educates UI.\n# Credentials to storage are expected in a secret \"john\" in the same namespace.\n# A Keycloak group, role, and client are created; user \"john\" must exist in Keycloak.\napiVersion: pkg.internal/v1beta1\nkind: Datalab\nmetadata:\n  name: s-john\nspec:\n  users:\n  - john\n  secretName: john\n  sessions:\n  - default\n  vcluster: false\n  files:\n  - git:\n      url: https://github.com/versioneer-tech/datalab-example\n      ref: origin/main\n    includePaths:\n    - /workshop/**\n    - /data/**\n    - /README.md\n    path: .\n</code></pre> <ul> <li>Preloads workshop materials from Git.  </li> <li>Activates the workshop tab in the UI for guided exercises.  </li> <li>Keycloak ensures only John has access to this environment and tooling.  </li> </ul>"},{"location":"how-to-guides/usage_concepts/#verifying-provisioning","title":"Verifying Provisioning","text":"<p>Once a <code>Datalab</code> claim has been applied, you can verify that the provisioning worked.</p>"},{"location":"how-to-guides/usage_concepts/#check-composite-status","title":"Check Composite Status","text":"<pre><code>kubectl get datalabs -n workspace\n</code></pre> <p>You should see all Datalabs <code>READY=True</code> once reconciliation is complete:</p> <pre><code>NAME       SYNCED   READY   COMPOSITION       AGE\ns-joe      True     True    datalab-educates  2m\ns-jeff     True     True    datalab-educates  2m\ns-jane     True     True    datalab-educates  2m\ns-john     True     True    datalab-educates  2m\n</code></pre> <p>Inspect details:</p> <pre><code>kubectl describe datalab s-jeff -n workspace\n</code></pre> <p>Look for conditions like <code>Ready=True</code> and any event messages.</p>"},{"location":"how-to-guides/usage_concepts/#find-the-storage-secret","title":"Find the Storage Secret","text":"<p>Each Datalab references a Secret in the same namespace via <code>spec.secretName</code>. For example, the claim <code>s-jeff</code> with <code>secretName: jeff</code> requires a Secret named <code>jeff</code>.</p> <pre><code>kubectl get secret jeff -n workspace -o yaml\n</code></pre> <p>Decode credentials (AWS-style):</p> <pre><code>kubectl get secret jeff -n workspace -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d; echo\nkubectl get secret jeff -n workspace -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d; echo\n</code></pre>"},{"location":"how-to-guides/usage_concepts/#summary","title":"Summary","text":"<ul> <li>A <code>Datalab</code> defines users, sessions, optional vcluster, quotas, and security policies.  </li> <li>Security controls combine Pod Security Standards, Kubernetes roles, and Docker privilege toggles.  </li> <li>Each Datalab requires a storage credential Secret.  </li> <li>Users must already exist in Keycloak; the Datalab provisions groups, memberships, a client, and a role.  </li> <li>Sessions may be long-lived (auto-created) or on-demand (user started).  </li> <li>Workshop files enable the Educates UI workshop tab.  </li> <li>Check <code>kubectl get datalabs</code> for readiness and confirm Secret and Keycloak resource creation.  </li> </ul>"},{"location":"reference-guides/api/","title":"API Reference","text":"<p>Packages:</p> <ul> <li>pkg.internal/v1beta1</li> </ul>"},{"location":"reference-guides/api/#pkginternalv1beta1","title":"pkg.internal/v1beta1","text":"<p>Resource Types:</p> <ul> <li>Datalab</li> </ul>"},{"location":"reference-guides/api/#datalab","title":"Datalab","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>A Datalab is a tenant-facing, namespaced composite resource. It defines ownership, membership, and optional file bundles to materialize in the environment.</p> Name Type Description Required apiVersion string pkg.internal/v1beta1 true kind string Datalab true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            Desired configuration of the datalab. true status object            Current observed state of the datalab. false"},{"location":"reference-guides/api/#datalabspec","title":"Datalab.spec","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Desired configuration of the datalab.</p> Name Type Description Required users []string            Users associated with this datalab. Default: [] true files []object            File bundles to fetch from remote sources and copy into the environment. Supports image, git, and http sources, path filtering, and optional credentials. Default: [] false quota object            Optional per-datalab session quota overrides. If a field is not specified here, the composition falls back to EnvironmentConfig at `spec.defaults.quota`, and then to hard defaults. Effective defaults (when neither XR nor EnvironmentConfig provides a value): memory=2Gi, storage=1Gi, budget=medium. false secretName string            Name of the Secret containing the credentials to access the storage associated with this Datalab. The Secret must exist in the same namespace as the Datalab. false security object            Optional per-datalab session security settings. If a field is not specified here, the composition falls back to EnvironmentConfig at `spec.defaults.security`, and then to hard defaults. Effective defaults (when neither XR nor EnvironmentConfig provides a value): policy=baseline, kubernetesAccess=true, kubernetesRole=edit. When policy is \"privileged\", Docker is automatically enabled with 20Gi storage. false sessions []string            Sessions to be started for this datalab. Default: [] false vcluster boolean            Whether to provision an isolated vcluster for each datalab session. Default: false false"},{"location":"reference-guides/api/#datalabspecfilesindex","title":"Datalab.spec.files[index]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required excludePaths []string            Glob patterns to exclude from the source. false git object            Git repository source configuration. false http object            HTTP source configuration for downloading an asset or archive. false image object            Container image source configuration. false includePaths []string            Glob patterns to include from the source. false newRootPath string            Subdirectory within the source to treat as the root. false path string            Destination directory for extracted files. Default: . false"},{"location":"reference-guides/api/#datalabspecfilesindexgit","title":"Datalab.spec.files[index].git","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Git repository source configuration.</p> Name Type Description Required lfsSkipSmudge boolean            If true, do not fetch Git LFS objects. false ref string            Branch, tag, or commit to fetch. false refSelection object            Resolve an explicit ref by semver selection. false secretRef object            Optional credentials for the Git server. false url string            Git repository URL (HTTPS or SSH). false verification object            GPG signature verification options. false"},{"location":"reference-guides/api/#datalabspecfilesindexgitrefselection","title":"Datalab.spec.files[index].git.refSelection","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Resolve an explicit ref by semver selection.</p> Name Type Description Required semver object false"},{"location":"reference-guides/api/#datalabspecfilesindexgitrefselectionsemver","title":"Datalab.spec.files[index].git.refSelection.semver","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required constraints string false prereleases object false"},{"location":"reference-guides/api/#datalabspecfilesindexgitrefselectionsemverprereleases","title":"Datalab.spec.files[index].git.refSelection.semver.prereleases","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifiers []string false"},{"location":"reference-guides/api/#datalabspecfilesindexgitsecretref","title":"Datalab.spec.files[index].git.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Optional credentials for the Git server.</p> Name Type Description Required name string            Name of a Secret with auth (ssh-privatekey/knownhosts or username/password). false namespace string            Namespace of the Secret. false"},{"location":"reference-guides/api/#datalabspecfilesindexgitverification","title":"Datalab.spec.files[index].git.verification","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>GPG signature verification options.</p> Name Type Description Required publicKeysSecretRef object false"},{"location":"reference-guides/api/#datalabspecfilesindexgitverificationpublickeyssecretref","title":"Datalab.spec.files[index].git.verification.publicKeysSecretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required name string            Secret containing GPG public keys. false namespace string            Namespace of the Secret. false"},{"location":"reference-guides/api/#datalabspecfilesindexhttp","title":"Datalab.spec.files[index].http","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>HTTP source configuration for downloading an asset or archive.</p> Name Type Description Required secretRef object            Optional basic-auth credentials for the HTTP server. false sha256 string            Optional checksum for verification of the downloaded asset. false url string            HTTP(S) URL to file or archive; archives are unpacked automatically. false"},{"location":"reference-guides/api/#datalabspecfilesindexhttpsecretref","title":"Datalab.spec.files[index].http.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Optional basic-auth credentials for the HTTP server.</p> Name Type Description Required name string            Secret containing username/password. false namespace string            Namespace of the Secret. false"},{"location":"reference-guides/api/#datalabspecfilesindeximage","title":"Datalab.spec.files[index].image","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Container image source configuration.</p> Name Type Description Required dangerousSkipTLSVerify boolean            Skip TLS verification when pulling from the registry. false secretRef object            Optional credentials for the image registry. false tagSelection object            Optional semantic-version tag selection policy. false url string            OCI image reference (e.g., ghcr.io/org/repo:tag or @sha256:...). false"},{"location":"reference-guides/api/#datalabspecfilesindeximagesecretref","title":"Datalab.spec.files[index].image.secretRef","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Optional credentials for the image registry.</p> Name Type Description Required name string            Name of a Secret containing registry credentials. false namespace string            Namespace of the Secret. false"},{"location":"reference-guides/api/#datalabspecfilesindeximagetagselection","title":"Datalab.spec.files[index].image.tagSelection","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Optional semantic-version tag selection policy.</p> Name Type Description Required semver object false"},{"location":"reference-guides/api/#datalabspecfilesindeximagetagselectionsemver","title":"Datalab.spec.files[index].image.tagSelection.semver","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required constraints string            Semver constraint string. false prereleases object false"},{"location":"reference-guides/api/#datalabspecfilesindeximagetagselectionsemverprereleases","title":"Datalab.spec.files[index].image.tagSelection.semver.prereleases","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> Name Type Description Required identifiers []string false"},{"location":"reference-guides/api/#datalabspecquota","title":"Datalab.spec.quota","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Optional per-datalab session quota overrides. If a field is not specified here, the composition falls back to EnvironmentConfig at <code>spec.defaults.quota</code>, and then to hard defaults. Effective defaults (when neither XR nor EnvironmentConfig provides a value): memory=2Gi, storage=1Gi, budget=medium.</p> Name Type Description Required budget enum            Namespace budget class determining available compute resources. Accepted values correspond to standard Educates resource budgets. Effective default (if not set here or in EnvironmentConfig): \"medium\". Enum: small, medium, large, x-large, xx-large, xxx-large false memory string            Memory request for the session environment as a Kubernetes quantity (e.g., \"2Gi\", \"512Mi\"). Effective default (if not set here or in EnvironmentConfig): \"2Gi\". false storage string            Storage size for the session as a Kubernetes quantity (e.g., \"1Gi\"). Effective default (if not set here or in EnvironmentConfig): \"1Gi\". false"},{"location":"reference-guides/api/#datalabspecsecurity","title":"Datalab.spec.security","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Optional per-datalab session security settings. If a field is not specified here, the composition falls back to EnvironmentConfig at <code>spec.defaults.security</code>, and then to hard defaults. Effective defaults (when neither XR nor EnvironmentConfig provides a value): policy=baseline, kubernetesAccess=true, kubernetesRole=edit. When policy is \"privileged\", Docker is automatically enabled with 20Gi storage.</p> Name Type Description Required kubernetesAccess boolean            Whether a Kubernetes service account token should be made available within the session. Effective default (if not set here or in EnvironmentConfig): true. false kubernetesRole enum            Session namespace RBAC role. Accepted values: \"admin\", \"edit\", \"view\". Effective default (if not set here or in EnvironmentConfig): \"edit\". Enum: admin, edit, view false policy enum            Pod Security Standard policy level. Accepted values: \"restricted\", \"baseline\", \"privileged\". Effective default (if not set here or in EnvironmentConfig): \"baseline\". Enum: restricted, baseline, privileged false"},{"location":"reference-guides/api/#datalabstatus","title":"Datalab.status","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Current observed state of the datalab.</p> Name Type Description Required sessions map[string]object            Map of session IDs and their current state. false"},{"location":"reference-guides/api/#datalabstatussessionskey","title":"Datalab.status.sessions[key]","text":"<p><sup><sup>\u21a9 Parent</sup></sup></p> <p>Observed state of a single datalab session.</p> Name Type Description Required url string            Public URL of the active session. false"}]}